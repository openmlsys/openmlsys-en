<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <title>
   2.3. Generating a Computational Graph — Machine Learning Systems: Design and Implementation 1.0.0 documentation
  </title>
  <link href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/sphinx_materialdesign_theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fontawesome/all.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fonts.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/basic.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/d2l.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/sphinx_highlight.js">
  </script>
  <script src="../_static/d2l.js">
  </script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <link href="../_static/favicon.png" rel="shortcut icon"/>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="schedule_of_computational_graph.html" rel="next" title="2.4. Scheduling and Executing Computational Tasks"/>
  <link href="components_of_computational_graph.html" rel="prev" title="2.2. Computational Graph Basics"/>
 </head>
 <body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer">
   <header class="mdl-layout__header mdl-layout__header--waterfall">
    <div class="mdl-layout__header-row">
     <nav class="mdl-navigation breadcrumb">
      <a class="mdl-navigation__link" href="index.html">
       <span class="section-number">
        2.
       </span>
       Computational Graph
      </a>
      <i class="material-icons">
       navigate_next
      </i>
      <a class="mdl-navigation__link is-active">
       <span class="section-number">
        2.3.
       </span>
       Generating a Computational Graph
      </a>
     </nav>
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <form action="../search.html" class="form-inline pull-sm-right" method="get">
       <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label class="mdl-button mdl-js-button mdl-button--icon" for="waterfall-exp" id="quick-search-icon">
         <i class="material-icons">
          search
         </i>
        </label>
        <div class="mdl-textfield__expandable-holder">
         <input class="mdl-textfield__input" id="waterfall-exp" name="q" placeholder="Search" type="text"/>
         <input name="check_keywords" type="hidden" value="yes"/>
         <input name="area" type="hidden" value="default"/>
        </div>
       </div>
       <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
        Quick search
       </div>
      </form>
      <a class="mdl-button mdl-js-button mdl-button--icon" href="../_sources/chapter_computational_graph/generation_of_computational_graph.rst.txt" id="button-show-source" rel="nofollow">
       <i class="material-icons">
        code
       </i>
      </a>
      <div class="mdl-tooltip" data-mdl-for="button-show-source">
       Show Source
      </div>
     </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <a class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-en">
       <i class="fab fa-github">
       </i>
       GitHub
      </a>
      <a class="mdl-navigation__link" href="https://openmlsys.github.io/">
       <i class="fas fa-external-link-alt">
       </i>
       中文版
      </a>
     </nav>
    </div>
   </header>
   <header class="mdl-layout__drawer">
    <!-- Title -->
    <span class="mdl-layout-title">
     <a class="title" href="../index.html">
      <img alt="Machine Learning Systems: Design and Implementation" class="logo" src="../_static/logo-with-text.png"/>
     </a>
    </span>
    <div class="globaltoc">
     <span class="mdl-layout-title toc">
      Table Of Contents
     </span>
     <nav class="mdl-navigation">
      <ul class="current">
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface/index.html">
         1. Preface
        </a>
       </li>
       <li class="toctree-l1 current">
        <a class="reference internal" href="index.html">
         2. Computational Graph
        </a>
        <ul class="current">
         <li class="toctree-l2">
          <a class="reference internal" href="background_and_functionality.html">
           2.1. Computational Graph Functions
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="components_of_computational_graph.html">
           2.2. Computational Graph Basics
          </a>
         </li>
         <li class="toctree-l2 current">
          <a class="current reference internal" href="#">
           2.3. Generating a Computational Graph
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="schedule_of_computational_graph.html">
           2.4. Scheduling and Executing Computational Tasks
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="summary.html">
           2.5. Chapter Summary
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="summary.html#further-reading">
           2.6. Further Reading
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </nav>
    </div>
   </header>
   <main class="mdl-layout__content" tabindex="0">
    <script src="../_static/sphinx_materialdesign_theme.js " type="text/javascript">
    </script>
    <header class="mdl-layout__drawer">
     <!-- Title -->
     <span class="mdl-layout-title">
      <a class="title" href="../index.html">
       <img alt="Machine Learning Systems: Design and Implementation" class="logo" src="../_static/logo-with-text.png"/>
      </a>
     </span>
     <div class="globaltoc">
      <span class="mdl-layout-title toc">
       Table Of Contents
      </span>
      <nav class="mdl-navigation">
       <ul class="current">
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface/index.html">
          1. Preface
         </a>
        </li>
        <li class="toctree-l1 current">
         <a class="reference internal" href="index.html">
          2. Computational Graph
         </a>
         <ul class="current">
          <li class="toctree-l2">
           <a class="reference internal" href="background_and_functionality.html">
            2.1. Computational Graph Functions
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="components_of_computational_graph.html">
            2.2. Computational Graph Basics
           </a>
          </li>
          <li class="toctree-l2 current">
           <a class="current reference internal" href="#">
            2.3. Generating a Computational Graph
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="schedule_of_computational_graph.html">
            2.4. Scheduling and Executing Computational Tasks
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="summary.html">
            2.5. Chapter Summary
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="summary.html#further-reading">
            2.6. Further Reading
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </nav>
     </div>
    </header>
    <div class="document">
     <div class="page-content" role="main">
      <div class="section" id="generating-a-computational-graph">
       <h1>
        <span class="section-number">
         2.3.
        </span>
        Generating a Computational Graph
        <a class="headerlink" href="#generating-a-computational-graph" title="Permalink to this heading">
         ¶
        </a>
       </h1>
       <p>
        In the previous section, we explored the ingredients of a computational
graph. Now let’s proceed to the next question — how is a computational
graph automatically generated? Machine learning frameworks support two
approaches to implementing computational graphs: static and dynamic. The
static approach builds a static (unchanging) graph based on information
such as the network topology and parameter variables described by the
frontend language. Because frontend languages are independent, static
graphs are especially suitable for model deployment (e.g., deploying a
facial recognition application on mobile devices).
       </p>
       <p>
        Unlike the static approach, the dynamic approach dynamically generates a
temporary graph based on the frontend description each time the model is
executed. Dynamic graphs are easy to debug, making it possible to
fine-tune models efficiently on the fly. Major machine learning
frameworks such as TensorFlow and MindSpore are compatible with both
approaches. And although PyTorch uses dynamic graphs, it also offers
dynamic-to-static conversion support for efficient model execution. To
choose the right approach for a specific task, we need to consider the
task requirements as well as the pros and cons of each approach.
       </p>
       <div class="section" id="static-graph">
        <h2>
         <span class="section-number">
          2.3.1.
         </span>
         Static Graph
         <a class="headerlink" href="#static-graph" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         The static graph approach decouples the definition and execution
processes. That is, a static graph is compiled before it is executed, as
shown in Figure
         <a class="reference internal" href="#static">
          <span class="std std-numref">
           Fig. 2.3.1
          </span>
         </a>
         .
        </p>
        <div class="figure align-default" id="id1">
         <span id="static">
         </span>
         <a class="reference internal image-reference" href="../_images/static.png">
          <img alt="../_images/static.png" src="../_images/static.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           Fig. 2.3.1
          </span>
          <span class="caption-text">
           Generating and executing a static graph
          </span>
          <a class="headerlink" href="#id1" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         When a model program is generated using the frontend language, the
machine learning framework first analyzes the model topology for
information such as the connections between network layers, parameter
variable settings, and loss functions. The framework then compiles the
model description into fixed code (i.e., a static computational graph)
that can be invoked and executed by the computing backend. In this case,
subsequent training or inference on this model is no longer
frontend-dependent. Specifically, when input data is fed into the static
graph, the operators in the graph are directly scheduled to hardware for
execution. And to improve hardware computational efficiency, we can also
convert a static graph into other equivalent structures through various
optimization strategies.
        </p>
        <p>
         The following code shows an example of generating and executing a simple
static graph. In the frontend definition phase, some machine learning
frameworks require developers to declare predefined configuration items
including tensor placeholders, loss functions, optimization functions,
network building and runtime environments, and network executors, as
well as in-graph control statements using control flow operators. The
design of machine learning frameworks has recently been improved to
provide easy-to-use APIs and a unified model building paradigm. For
example, MindSpore enables unified frontend programming representations
featuring dynamic and static integration. To illustrate, let’s consider
the following simple model.
        </p>
        <div class="highlight-python notranslate">
         <div class="highlight">
          <pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">flag</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">flag</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span>
</pre>
         </div>
        </div>
        <p>
         The machine learning framework does not load input data when generating
a static graph. Instead,
         <em>
          placeholder
         </em>
         tensors are used to hold places
of input data. In the static graph defined the above code, we need to
create a placeholder for input
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X}\)
         </span>
         in line 1. Because
no actual input is fed into the model during static graph generation,
the control flow defined in line 2 cannot make control decisions at
build time. As such, we need to add the control flow operator and the
computational subgraph of each branch to the static graph. When the
model receives actual inputs during runtime, different branches are
taken (by running the corresponding computational subgraphs) depending
on different inputs. However, not all machine learning frameworks are
able to compile Python control flows as their static graph equivalents.
In order to implement control flows in this case, we can use the control
primitives provided by the framework.
        </p>
        <div class="figure align-default" id="id2">
         <span id="staticgen">
         </span>
         <a class="reference internal image-reference" href="../_images/static_gen.png">
          <img alt="../_images/static_gen.png" src="../_images/static_gen.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           Fig. 2.3.2
          </span>
          <span class="caption-text">
           Generating a static graph
          </span>
          <a class="headerlink" href="#id2" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         Static computational graphs offer two distinct advantages. First, they
yield better performance with less memory. When building a static graph,
the machine learning framework acquires the complete model topology
containing global information of the model, which facilitates the
formulation of graph optimization strategies (e.g., the operator fusion
strategy that fuses two or more operators into a larger one). As shown
in Figure
         <a class="reference internal" href="#staticgen">
          <span class="std std-numref">
           Fig. 2.3.2
          </span>
         </a>
         , the Add and ReLU operators are fused
into one operator to reduce the loads/stores of intermediate results and
low-level scheduling overhead, thereby improving the execution
performance and efficiency with a lower memory footprint. Static graphs
allow for many optimization strategies at build time, which we will
discuss in later sections.
        </p>
        <p>
         Second, by converting static graphs into executable code within the
machine learning framework, we can directly deploy our models on various
hardware platforms to provide efficient inference services. Also, we can
store static graphs using serialization techniques for future execution
(either model training or inference), eliminating the need to rebuild
the frontend source code from scratch every time before execution.
        </p>
        <p>
         Once the frontend code of the model is compiled into a static graph, the
graph structure is fixed. If we introduce any optimizations to the
graph, the optimized code can differ significantly from the original.
However, the optimized code is not intuitively visible, meaning that it
is sometimes impossible to locate a runtime error based on the returned
code line number in the optimized code. Consider a simple case. Assuming
that the Add and ReLU operators in the above code have been fused for
optimization, if a runtime error related to the fused operator is
reported, it would be hard for us to determine the exact error location
(Add or ReLU).
        </p>
        <p>
         In addition, in the daunting process of model debugging and testing,
intermediate results cannot be printed in real time. To make this
happen, we need to insert additional code to the source code and then
recompile the source code for execution, making debugging less
efficient. By contrast, the dynamic graph approach offers more
flexibility.
        </p>
       </div>
       <div class="section" id="dynamic-graph">
        <h2>
         <span class="section-number">
          2.3.2.
         </span>
         Dynamic Graph
         <a class="headerlink" href="#dynamic-graph" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         Figure
         <a class="reference internal" href="#dynamic">
          <span class="std std-numref">
           Fig. 2.3.3
          </span>
         </a>
         shows the principle of the dynamic graph
approach. A dynamic graph is defined as it runs. The frontend
interpreter parses the graph code and the machine learning framework
distributes the operators in the graph to the backend for just-in-time
(JIT) execution. Adopting the user-friendly imperative programming
paradigm, the dynamic graph approach allows developers to create neural
network models at the frontend and is therefore favored by a vast number
of deep learning researchers.
        </p>
        <div class="figure align-default" id="id3">
         <span id="dynamic">
         </span>
         <a class="reference internal image-reference" href="../_images/eager.png">
          <img alt="../_images/eager.png" src="../_images/eager.png" style="width: 600px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           Fig. 2.3.3
          </span>
          <span class="caption-text">
           动态图原理
          </span>
          <a class="headerlink" href="#id3" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         Next, we reuse the pseudocode in the previous section to compare the
dynamic and static graph approaches.
        </p>
        <p>
         While these two approaches differ only slightly in their frontend
representations, they differ dramatically in terms of their compilation
and execution mechanisms. Unlike the static graph approach, the dynamic
graph approach calls the built-in operator distribution function of the
machine learning framework through the Python API to distribute Python
operators to the hardware backend (e.g., CPU, GPU, or NPU) for
accelerated computing, which then returns the computational result to
the frontend. This process does not generate a static computational
graph. Instead, the framework describes the model topology using the
frontend language, schedules and executes the model based on
computational dependencies, and dynamically generates a temporary graph.
        </p>
        <p>
         Figure
         <a class="reference internal" href="#dynamicgen">
          <span class="std std-numref">
           Fig. 2.3.4
          </span>
         </a>
         shows the process of generating a dynamic
graph.
        </p>
        <div class="figure align-default" id="id4">
         <span id="dynamicgen">
         </span>
         <a class="reference internal image-reference" href="../_images/eager-gen.png">
          <img alt="../_images/eager-gen.png" src="../_images/eager-gen.png" style="width: 700px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           Fig. 2.3.4
          </span>
          <span class="caption-text">
           动态生成
          </span>
          <a class="headerlink" href="#id4" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         Forward computation is run through the neural network in the sequence
defined by the model declaration. Once the model receives input
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X}\)
         </span>
         , the machine learning framework starts to
generate a dynamic graph by adding the input node to the graph and
sending the data to the downstream node. The control flow (if available)
makes a data flow decision immediately. For example, in Figure
         <a class="reference internal" href="#dynamicgen">
          <span class="std std-numref">
           Fig. 2.3.4
          </span>
         </a>
         , if the conditional returns true, only the Matmul
operator node with respect to tensor
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W1}\)
         </span>
         is added to
the graph. Then, the machine learning framework inserts the Add and ReLU
operator nodes based on the operator sequence and computational
dependencies defined in the code. For each newly added operator node,
the machine learning framework distributes and executes the operator,
returns the computational result, and prepares to pass the result to the
next node. When forward computation resumes, the last dynamic graph
becomes invalid and a new dynamic graph is created according to current
input and control decision. In contrast with a static graph that
represents the entire model described in the frontend language, a
dynamic graph is generated on the fly as the control flow and data flow
evolve over time. For this reason, the machine learning framework has
few opportunities to optimize the model in the dynamic graph setting.
        </p>
        <p>
         In the static graph setting, as the model definition is entirely
available, a complete forward computational graph and a complete
backward computational graph can be constructed simultaneously. However,
in the dynamic graph setting, gradients are calculated for
backpropagation as the forward pass proceeds. Specifically, the machine
learning framework collects information of each backward operator and
tensor participating in gradient computation based on the information of
each operator called in the forward pass. Once the forward pass ends,
the operator and tensor information for backpropagation becomes
available. With this information, the machine learning framework creates
a backward computational graph and runs it on hardware to complete
gradient computation and parameter update.
        </p>
        <p>
         As shown in Figure
         <a class="reference internal" href="#dynamicgen">
          <span class="std std-numref">
           Fig. 2.3.4
          </span>
         </a>
         ，when the Matmul operator
with respect to tensor
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W1}\)
         </span>
         is called, the framework
runs the Matmul operator to calculate the product of inputs
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X}\)
         </span>
         and
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W1}\)
         </span>
         , and then records the
operator and tensor
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X}\)
         </span>
         that will participate in
backpropagation based on the backward computation process
Grad_
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W1}\)
         </span>
         =Grad_
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}*\boldsymbol{X}\)
         </span>
         ,
thereby completing the forward pass and producing a backward
computational graph.
        </p>
        <p>
         Although the optimization techniques useful in the static graph setting
do not work for dynamic graphs (because the complete network structure
is unknown until the dynamic graph runs), researchers and developers can
easily analyze errors and debug results during model testing and
optimization. This is made possible by dynamic graphs supporting JIT
computing and returning computational results immediately with the
execution of each statement.
        </p>
        <p>
         Also, the dynamic graph approach enables flexible execution using native
control flows provided by the frontend — unlike static graphs, which
involve complex control flows along with programming and debugging
difficulties. Consequently, the dynamic graph approach lowers the
barriers to programming for beginners while also improving the iteration
efficiency of algorithm development and model optimization.
        </p>
       </div>
       <div class="section" id="dynamic-graph-vs-static-graph">
        <h2>
         <span class="section-number">
          2.3.3.
         </span>
         Dynamic Graph vs. Static Graph
         <a class="headerlink" href="#dynamic-graph-vs-static-graph" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         The two approaches for implementing computational graphs have their pros
and cons, as described in Table
         <a class="reference internal" href="#cmp-dynamic-static">
          <span class="std std-numref">
           Table 2.3.1
          </span>
         </a>
         。
        </p>
        <span id="cmp-dynamic-static">
        </span>
        <table class="docutils align-default" id="id5" style="margin-left:auto;margin-right:auto;margin-top:10px;margin-bottom:20px;">
         <caption>
          <span class="caption-number">
           Table 2.3.1
          </span>
          <span class="caption-text">
           Static graph vs. dynamic graph
          </span>
          <a class="headerlink" href="#id5" title="Permalink to this table">
           ¶
          </a>
         </caption>
         <colgroup>
          <col style="width: 16%"/>
          <col style="width: 42%"/>
          <col style="width: 42%"/>
         </colgroup>
         <thead>
          <tr class="row-odd">
           <th class="head">
            <p>
             Feature
            </p>
           </th>
           <th class="head">
            <p>
             Static Graph
            </p>
           </th>
           <th class="head">
            <p>
             Dynamic Graph
            </p>
           </th>
          </tr>
         </thead>
         <tbody>
          <tr class="row-even">
           <td>
            <p>
             On-the-fl
y
intermedi
ate
results
            </p>
           </td>
           <td>
            <p>
             No
            </p>
           </td>
           <td>
            <p>
             Yes
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             Code
debugging
            </p>
           </td>
           <td>
            <p>
             Difficult
            </p>
           </td>
           <td>
            <p>
             Easy
            </p>
           </td>
          </tr>
          <tr class="row-even">
           <td>
            <p>
             Control
flow
implement
ation
            </p>
           </td>
           <td>
            <p>
             Specialized syntax
            </p>
           </td>
           <td>
            <p>
             Frontend syntax
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             Performan
ce
            </p>
           </td>
           <td>
            <p>
             Better, supporting wide
optimization strategies
            </p>
           </td>
           <td>
            <p>
             Poor, supporting limited
graph optimizations
            </p>
           </td>
          </tr>
          <tr class="row-even">
           <td>
            <p>
             Memory
footprint
            </p>
           </td>
           <td>
            <p>
             Low
            </p>
           </td>
           <td>
            <p>
             High
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             Direct
deploymen
t
            </p>
           </td>
           <td>
            <p>
             Yes
            </p>
           </td>
           <td>
            <p>
             No
            </p>
           </td>
          </tr>
         </tbody>
        </table>
        <p>
         Compared with the dynamic graph approach, the static graph approach
seems to be less user-friendly to developers because intermediate
results are not available on the fly, code debugging is difficult, and
implementing control flows is complex. However, static graphs ensure
higher execution performance than dynamic graphs. See the example in
following Code.
        </p>
        <div class="highlight-python notranslate">
         <div class="highlight">
          <pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">):</span>
    <span class="n">Y1</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span>
    <span class="n">Y2</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y1</span> <span class="o">+</span> <span class="n">Y2</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre>
         </div>
        </div>
        <p>
         If the static approach is used to implement in the above code, the
machine learning framework creates a complete computational graph.
Because tensors
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y_1}\)
         </span>
         and
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y_2}\)
         </span>
         are computed independently from each other, we can implement automatic
parallelism on them in order to improve the computational efficiency.
Furthermore, the static approach allows many more optimization
strategies to improve efficiency while also lowering memory footprint,
for example, fusing operators Add and ReLU to reduce the loads and
stores of the intermediate variable
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}\)
         </span>
         . Conversely,
if the dynamic approach is used without a manually configured
parallelism strategy, the machine learning framework is unaware of the
independence between operators due to the lack of a complete
computational graph. Consequently, the framework has to execute the
operators, including Add and ReLU, in a defined order and store the
intermediate variable
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}\)
         </span>
         . To further reduce memory
footprint, the static approach narrows down the intermediate variables
to be stored for backpropagation beforehand in the forward pass, based
on the forward and backward computational graphs defined prior to
execution. This is not feasible in the dynamic approach, where the
backward computational graph is defined only after the forward pass is
complete. As such, more intermediate variables have to be stored in the
forward pass to ensure the backpropagation efficiency, resulting in
higher memory footprint.
        </p>
        <p>
         To choose one approach over the other, we should consider their pros and
cons in addition to analyzing specific task requirements. For academic
research purposes or in the model design and debugging phases, the
dynamic graph approach is suggested because it allows for quick testing
of experimental ideas and iterative update of the model structure. In
other cases where the model structure is determinant, to accelerate the
training process or deploy a model on specific hardware, using the
static graph approach offers higher efficiency.
        </p>
       </div>
       <div class="section" id="conversion-between-and-combination-of-dynamic-and-static-graphs">
        <h2>
         <span class="section-number">
          2.3.4.
         </span>
         Conversion Between and Combination of Dynamic and Static Graphs
         <a class="headerlink" href="#conversion-between-and-combination-of-dynamic-and-static-graphs" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         Dynamic graphs are easy to debug and suitable for model design and
testing, whereas static graphs improve execution efficiency and shorten
model training time. Is there a way for the machine learning framework
to combine the merits of both approaches? Major machine learning
frameworks, such as TensorFlow, MindSpore, PyTorch, and PaddlePaddle,
have added support to convert between dynamic and static graphs,
allowing developers to program using the dynamic graph approach and
letting the framework automatically convert the code to a static
equivalent for execution.
        </p>
        <p>
         Table
         <a class="reference internal" href="#dynamic-static-switch">
          <span class="std std-numref">
           Table 2.3.2
          </span>
         </a>
         lists the APIs for dynamic graph
to static graph conversion provided by major frameworks.
        </p>
        <span id="dynamic-static-switch">
        </span>
        <table class="docutils align-default" id="id6" style="margin-left:auto;margin-right:auto;margin-top:10px;margin-bottom:20px;">
         <caption>
          <span class="caption-number">
           Table 2.3.2
          </span>
          <span class="caption-text">
           Dynamic graph to static graph conversion support of major
frameworks
          </span>
          <a class="headerlink" href="#id6" title="Permalink to this table">
           ¶
          </a>
         </caption>
         <colgroup>
          <col style="width: 26%"/>
          <col style="width: 74%"/>
         </colgroup>
         <thead>
          <tr class="row-odd">
           <th class="head">
            <p>
             Framework
            </p>
           </th>
           <th class="head">
            <p>
             Dynamic Graph to Static Graph Conversion
            </p>
           </th>
          </tr>
         </thead>
         <tbody>
          <tr class="row-even">
           <td>
            <p>
             TensorFlow
            </p>
           </td>
           <td>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               @tf_function
              </span>
             </code>
             : builds a static graph by
tracing operators,，where AutoGraph automatically
transforms a control flow to the equivalent
static statement.
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             MindSpore
            </p>
           </td>
           <td>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               context.set_context(mode=context.PYNATIVE_MODE)
              </span>
              <span class="pre">
               ``\
              </span>
              <span class="pre">
               dynamic
              </span>
              <span class="pre">
               graph
              </span>
              <span class="pre">
               mode,
              </span>
              <span class="pre">
               \
              </span>
              <span class="pre">
               ``context.set_context(mode=context.GRAPH_MODE)`
              </span>
              <span class="pre">
               `
              </span>
              <span class="pre">
               static
              </span>
              <span class="pre">
               graph
              </span>
              <span class="pre">
               mode,
              </span>
              <span class="pre">
               \
              </span>
              <span class="pre">
               ``@ms_function
              </span>
             </code>
             : builds a
static graph from source code.
            </p>
           </td>
          </tr>
          <tr class="row-even">
           <td>
            <p>
             PyTorch
            </p>
           </td>
           <td>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.jit.script()
              </span>
             </code>
             : builds a static graph
from source code.
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.jit.trace()
              </span>
             </code>
             : builds
a static graph by tracing operators.
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             PaddlePaddle
            </p>
           </td>
           <td>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               paddle.jit.to_static()
              </span>
             </code>
             : builds a static graph
from source
code.
             <code class="docutils literal notranslate">
              <span class="pre">
               paddle.jit.TracedLayer.trace()
              </span>
             </code>
             : builds
a static graph by tracing operators.
            </p>
           </td>
          </tr>
         </tbody>
        </table>
        <p>
         These dynamic-to-static conversion methods fall into the following two
categories:
        </p>
        <ul class="simple">
         <li>
          <p>
           <strong>
            Tracing
           </strong>
           : A static graph is built by tracing operator scheduling
in a dynamic graph.
          </p>
         </li>
         <li>
          <p>
           <strong>
            Source code transformation
           </strong>
           : The frontend code is inspected and
built as static graph code. And the static graph executor is
automatically called to run the static graph.
          </p>
         </li>
        </ul>
        <p>
         The
         <strong>
          tracing
         </strong>
         method goes through two simple phases. The first is to
generate a dynamic graph, following a workflow similar to that shown in
Figure
         <a class="reference internal" href="#dynamicgen">
          <span class="std std-numref">
           Fig. 2.3.4
          </span>
         </a>
         . The machine learning framework runs the
created dynamic graph and traces the data flow and operator scheduling
in the dynamic graph to produce a static graph. Note that the dynamic
graph is not destroyed; instead, it is preserved as a static graph for
subsequent execution. As the machine learning framework finishes
executing the dynamic graph, a static graph is produced. In the second
phase when the model is called again, the machine learning framework
runs the static graph for computation. The tracing technique only traces
the operators scheduled when the dynamic graph is run for the first
time. However, if the model has a data-dependent conditional, only one
branch of the conditional can be traced — the traced graph would be
unable to take alternate branches. Similarly, the traced graph cannot
include every iteration if there is a data-dependent loop.
        </p>
        <p>
         Unlike dynamic graph code which is parsed and executed by the frontend
interpreter, a static graph must be first created by the graph compiler
of the machine learning framework before execution. Because the graph
compiler cannot directly deal with dynamic graph code, the source code
transformation–based method is introduced to convert the dynamic graph
code into static code description.
        </p>
        <p>
         The
         <strong>
          source code transformation
         </strong>
         –based method can overcome the
drawbacks involved in the tracing method and also consists of two
phases, as shown in Figure
         <a class="reference internal" href="#ast">
          <span class="std std-numref">
           Fig. 2.3.5
          </span>
         </a>
         . The first involves lexical
and syntax analysis. Specifically, the lexical analyzer scans and
analyzes every character in the dynamic graph code, splits the source
text by removing any white spaces or comments, and returns a stream of
tokens. Then, the syntax analyzer or parser analyzes the token stream,
eliminates any errors, and generates a parse tree as the output of the
phase. In the second phase, the built-in translators of the machine
learning framework scan and translate each part of the abstract syntax
tree to map the grammatical structures from dynamic graph format into
static graph format. Any control flow written in the frontend language
is transformed into the corresponding static graph API in this phase, so
as to include every branch of the control flow in the resulting graph.
Next, we can easily generate static graph code from the translated
syntax tree.
        </p>
        <div class="figure align-default" id="id7">
         <span id="ast">
         </span>
         <a class="reference internal image-reference" href="../_images/ast.png">
          <img alt="../_images/ast.png" src="../_images/ast.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           Fig. 2.3.5
          </span>
          <span class="caption-text">
           Source code transformation
          </span>
          <a class="headerlink" href="#id7" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         To improve the computational efficiency, we can transform the entire
model graph for fast deployment on hardware. Alternatively, we can
consider transforming some of the model functions into static subgraphs
and embedding them into the global dynamic graph as individual
operators, so that these exact functions would run in the form of static
graphs at execution time. This not only improves computational
efficiency but also retains flexibility for code debugging.
        </p>
        <p>
         Code as follow shows a simple model, which can be built into a dynamic
graph as a whole. In this example, we transform the
         <code class="docutils literal notranslate">
          <span class="pre">
           add_and_relu
          </span>
         </code>
         module into a static subgraph. The model runs on the input data in a
predefined sequence, resulting in a temporary dynamic graph. When the
         <code class="docutils literal notranslate">
          <span class="pre">
           Y=add_and_relu(Y,b)
          </span>
         </code>
         statement is executed, the machine learning
framework automatically runs the static subgraph transformed from the
module, achieving a performance gain by combining the advantages of
dynamic and static graphs.
        </p>
        <div class="highlight-python notranslate">
         <div class="highlight">
          <pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_and_relu</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span>

<span class="k">def</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">flag</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">flag</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">add_and_relu</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span>
</pre>
         </div>
        </div>
        <p>
         Dynamic-to-static conversion is mostly found in the model deployment
stage, as a workaround to the hardware constraints on dynamic graph
deployment, which requires the frontend model definition code for
topology discovery in addition to the file of already-trained
parameters. To remove the frontend dependency, once model training in
dynamic graph mode is complete, we may convert the model into static
graph format and serialize the model and parameter files, thereby
expanding the list of supported hardware.
        </p>
       </div>
      </div>
     </div>
     <div class="side-doc-outline">
      <div class="side-doc-outline--content">
       <div class="localtoc">
        <p class="caption">
         <span class="caption-text">
          Table Of Contents
         </span>
        </p>
        <ul>
         <li>
          <a class="reference internal" href="#">
           2.3. Generating a Computational Graph
          </a>
          <ul>
           <li>
            <a class="reference internal" href="#static-graph">
             2.3.1. Static Graph
            </a>
           </li>
           <li>
            <a class="reference internal" href="#dynamic-graph">
             2.3.2. Dynamic Graph
            </a>
           </li>
           <li>
            <a class="reference internal" href="#dynamic-graph-vs-static-graph">
             2.3.3. Dynamic Graph vs. Static Graph
            </a>
           </li>
           <li>
            <a class="reference internal" href="#conversion-between-and-combination-of-dynamic-and-static-graphs">
             2.3.4. Conversion Between and Combination of Dynamic and Static Graphs
            </a>
           </li>
          </ul>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <div class="clearer">
     </div>
    </div>
    <div class="pagenation">
     <a accesskey="P" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="components_of_computational_graph.html" id="button-prev" role="botton">
      <i class="pagenation-arrow-L fas fa-arrow-left fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Previous
       </span>
       <div>
        2.2. Computational Graph Basics
       </div>
      </div>
     </a>
     <a accesskey="N" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="schedule_of_computational_graph.html" id="button-next" role="botton">
      <i class="pagenation-arrow-R fas fa-arrow-right fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Next
       </span>
       <div>
        2.4. Scheduling and Executing Computational Tasks
       </div>
      </div>
     </a>
    </div>
   </main>
  </div>
 </body>
</html>
